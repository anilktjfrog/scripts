#!/usr/bin/env python3

"""
JFrog Artifactory Repository Comparison Tool
===========================================

This script compares artifacts between source and target JFrog Artifactory instances
and generates transfer commands using curl instead of JFrog CLI.

Features:
- Compares files between source and target repositories
- Filters out auto-generated files
- Generates curl-based transfer commands
- Supports parallel processing
- Comprehensive logging
- Configuration file support (required)

Usage:
    python compare_repos.py --config config.json
"""

import argparse
import concurrent.futures
import json
import logging
import os
import re
import signal
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple
import requests
from urllib.parse import urljoin, quote

# Suppress known Python 3.13 threading cleanup warnings
warnings.filterwarnings("ignore", message=".*context manager protocol.*")

# Additional Python 3.13 threading cleanup issue suppression
import threading
import atexit


def cleanup_threads():
    """Clean up threads properly on exit."""
    try:
        # Force cleanup of any remaining threads
        for thread in threading.enumerate():
            if thread is not threading.current_thread() and thread.daemon:
                try:
                    thread.join(timeout=0.1)
                except:
                    pass
    except:
        pass


# Register cleanup function
atexit.register(cleanup_threads)

# Global flag for graceful shutdown
_shutdown_requested = False


def signal_handler(signum, frame):
    """Handle interrupt signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    print("\nShutdown requested, cleaning up...", file=sys.stderr)


class SafeThreadPoolExecutor:
    """Thread pool executor with better cleanup for Python 3.13."""

    def __init__(self, max_workers):
        self.max_workers = max_workers
        self.executor = None

    def __enter__(self):
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        )
        return self.executor

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            try:
                # Cancel all pending futures
                for future in getattr(self.executor, "_threads_queues", {}).keys():
                    try:
                        future.cancel()
                    except:
                        pass

                # Shutdown with wait=False to avoid blocking
                self.executor.shutdown(wait=False)

                # Give threads a moment to finish
                time.sleep(0.1)

            except Exception as e:
                # Silently handle any cleanup issues
                pass
            finally:
                self.executor = None


class ArtifactoryComparer:
    """
    Main class for comparing artifacts between JFrog Artifactory instances.
    """

    def __init__(
        self,
        config_file: str,
        repos_file: str = None,
        repo_type: str = "local",
        command_type: str = "curl",
    ):
        """
        Initialize the comparer with configuration.

        Args:
            config_file: Path to configuration file (required)
            repos_file: Path to file containing repository names to compare
            repo_type: Type of repositories to fetch (local, remote, virtual, federated). Default: 'local'
            command_type: Type of commands to generate (curl, jfrog). Default: 'curl'
        """
        self.config = self._load_config(config_file)
        self.repos_file = repos_file
        self.repo_type = repo_type
        self.command_type = command_type
        self.output_dir = f"out_{int(time.time())}"

        # Create output directory
        Path(self.output_dir).mkdir(exist_ok=True)

        self.logger = self._setup_logging()

        # Files that are auto-generated by JFrog Artifactory and should be ignored
        self.skipped_files = {
            "repository.catalog",
            "maven-metadata.xml",
            "Packages.bz2",
            ".gemspec.rz",
            "Packages.gz",
            "Release",
            ".json",
            "Packages",
            "by-hash",
            "filelists.xml.gz",
            "other.xml.gz",
            "primary.xml.gz",
            "repomd.xml",
            "repomd.xml.asc",
            "repomd.xml.key",
        }

        # Patterns for auto-generated paths that should be ignored
        self.skipped_patterns = [
            r"^\.npm",
            r"^\.jfrog",
            r"^\.pypi",
            r"^\.composer",
            r"^index\.yaml$",
            r"^versions$",
            r"_uploads",
        ]

        self.logger.info(
            f"Initialized repository comparer with output directory: {self.output_dir}"
        )

    def _load_config(self, config_file: str) -> Dict:
        """
        Load configuration from file.

        Args:
            config_file: Path to JSON configuration file (required)

        Returns:
            Dictionary containing configuration
        """
        if not config_file or not os.path.exists(config_file):
            raise FileNotFoundError(
                f"Configuration file is required and must exist: {config_file}"
            )

        with open(config_file, "r") as f:
            config = json.load(f)

        # Set default values only if they don't exist in config
        if "parallel_workers" not in config:
            config["parallel_workers"] = 20
        if "request_timeout" not in config:
            config["request_timeout"] = (
                120  # Increased timeout for AQL queries on large repos
            )
        if "retry_attempts" not in config:
            config["retry_attempts"] = 3

        # Validate that we have the required configuration
        if not config.get("source_server") or not config.get("target_server"):
            raise ValueError(
                "Configuration file must contain 'source_server' and 'target_server' settings"
            )

        return config

    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration."""
        log_level = self.config.get("log_level", "INFO").upper()
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

        # Configure root logger
        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_format,
            handlers=[
                logging.FileHandler(f"{self.output_dir}/compare_repos.log"),
                logging.StreamHandler(sys.stdout),
            ],
        )

        return logging.getLogger(__name__)

    def _get_auth_headers(self, server_id: str) -> Dict[str, str]:
        """Get authentication headers for the specified server."""
        server_config = self.config["servers"][server_id]
        headers = {"Content-Type": "application/json"}

        if server_config.get("token"):
            headers["Authorization"] = f"Bearer {server_config['token']}"
        elif server_config.get("username") and server_config.get("password"):
            import base64

            credentials = base64.b64encode(
                f"{server_config['username']}:{server_config['password']}".encode()
            ).decode()
            headers["Authorization"] = f"Basic {credentials}"
        else:
            self.logger.warning(f"No authentication configured for server {server_id}")

        return headers

    def _make_request(
        self,
        server_id: str,
        endpoint: str,
        method: str = "GET",
        data: str = None,
        params: Dict = None,
    ) -> requests.Response:
        """Make HTTP request to JFrog Artifactory API."""
        server_config = self.config["servers"][server_id]

        # Properly join base URL with endpoint to handle /artifactory paths correctly
        base_url = server_config["url"].rstrip("/")
        endpoint = endpoint.lstrip("/")
        url = f"{base_url}/{endpoint}"

        headers = self._get_auth_headers(server_id)

        timeout = self.config.get("request_timeout", 30)
        retry_attempts = self.config.get("retry_attempts", 3)

        for attempt in range(retry_attempts):
            try:
                if method.upper() == "GET":
                    response = requests.get(
                        url, headers=headers, params=params, timeout=timeout
                    )
                elif method.upper() == "POST":
                    if data:
                        headers["Content-Type"] = "text/plain"
                    response = requests.post(
                        url, headers=headers, data=data, timeout=timeout
                    )
                else:
                    raise ValueError(f"Unsupported HTTP method: {method}")

                response.raise_for_status()
                return response

            except requests.exceptions.RequestException as e:
                self.logger.warning(
                    f"Request attempt {attempt + 1} failed for {url}: {str(e)}"
                )
                if attempt == retry_attempts - 1:
                    raise
                time.sleep(2**attempt)  # Exponential backoff

    def get_repositories(
        self, server_id: str, use_type_filter: bool = True
    ) -> List[str]:
        """
        Get list of repositories from Artifactory.

        Args:
            server_id: Server identifier
            use_type_filter: Whether to apply repository type filter (default: True)
        """
        self.logger.info(f"Fetching repositories from {server_id}")

        try:
            # Only add type parameter if use_type_filter is True
            params = {"type": self.repo_type} if use_type_filter else {}

            response = self._make_request(server_id, "/api/repositories", params=params)
            repositories = [repo["key"] for repo in response.json()]

            if use_type_filter:
                self.logger.info(
                    f"Found {len(repositories)} {self.repo_type} repositories in {server_id}"
                )
            else:
                self.logger.info(
                    f"Found {len(repositories)} repositories in {server_id}"
                )
            return repositories
        except Exception as e:
            self.logger.error(
                f"Failed to fetch repositories from {server_id}: {str(e)}"
            )
            raise

    def read_repositories_from_file(self, repos_file: str) -> List[str]:
        """
        Read repository names from a text file.

        Args:
            repos_file: Path to text file containing repository names (one per line)

        Returns:
            List of repository names
        """
        if not os.path.exists(repos_file):
            raise FileNotFoundError(f"Repository file not found: {repos_file}")

        repositories = []
        try:
            with open(repos_file, "r") as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    # Skip empty lines and comments
                    if line and not line.startswith("#"):
                        repositories.append(line)

            self.logger.info(f"Read {len(repositories)} repositories from {repos_file}")
            return repositories

        except Exception as e:
            self.logger.error(
                f"Failed to read repositories from {repos_file}: {str(e)}"
            )
            raise

    def filter_existing_repositories(
        self, requested_repos: List[str], available_repos: List[str]
    ) -> List[str]:
        """
        Filter requested repositories to only include those that exist on the source server.

        Args:
            requested_repos: List of repository names requested for comparison
            available_repos: List of repository names available on source server

        Returns:
            List of repositories that exist and will be compared
        """
        available_set = set(available_repos)
        existing_repos = []
        missing_repos = []

        for repo in requested_repos:
            if repo in available_set:
                existing_repos.append(repo)
            else:
                missing_repos.append(repo)

        if missing_repos:
            self.logger.warning(
                f"The following repositories were not found on source server: {missing_repos}"
            )

        self.logger.info(
            f"Found {len(existing_repos)} out of {len(requested_repos)} requested repositories"
        )
        return existing_repos

    def get_repository_files(
        self, server_id: str, repo_name: str
    ) -> List[Tuple[str, str]]:
        """
        Get list of ALL files in repository with their SHA256 checksums using AQL with pagination.

        This method uses Artifactory Query Language (AQL) with pagination to efficiently
        handle very large repositories that may contain millions of files.

        Uses Artifactory AQL API:
        POST /api/search/aql
        Content-Type: text/plain

        AQL Query: items.find({"repo": "repo_name"}).include("name", "path", "size", "sha256")

        Pagination is handled automatically by checking the 'range' field in the response:
        - start_pos: Starting position of current batch
        - end_pos: Ending position of current batch
        - total: Total number of items available
        - limit: Maximum items per request (default: 500000)

        Args:
            server_id: Server identifier
            repo_name: Repository name

        Returns:
            List of tuples (file_path, sha256_checksum) for ALL files in the repository
        """
        self.logger.info(
            f"Fetching files from repository {repo_name} on {server_id} using AQL"
        )

        all_files = []
        offset = 0
        limit = 100000  # Process in batches of 100k files
        total_files = None

        try:
            while True:
                # Construct AQL query with pagination
                aql_query = f"""items.find({{"repo": "{repo_name}"}}).include("name", "path", "sha256").offset({offset}).limit({limit})"""

                self.logger.debug(f"AQL Query: {aql_query}")
                self.logger.debug(f"Fetching batch: offset={offset}, limit={limit}")

                # Make AQL request
                response = self._make_request(
                    server_id,
                    "/api/search/aql",
                    method="POST",
                    data=aql_query,
                )

                data = response.json()

                # Extract results and range information
                results = data.get("results", [])
                range_info = data.get("range", {})

                if total_files is None:
                    total_files = range_info.get("total", 0)
                    self.logger.info(
                        f"Repository {repo_name} contains {total_files} total files"
                    )

                # Process current batch
                batch_files = []
                for item in results:
                    if not isinstance(item, dict):
                        continue

                    # Construct full file path
                    path = item.get("path", "")
                    name = item.get("name", "")
                    sha256 = item.get("sha256", "")

                    if not name or not sha256:
                        continue

                    # Build full file path
                    if path and path != ".":
                        full_path = f"{path}/{name}"
                    else:
                        full_path = name

                    # Skip files that should be ignored (filter during AQL processing)
                    if self.should_skip_file(full_path):
                        continue

                    batch_files.append((full_path, sha256))

                all_files.extend(batch_files)

                # Log progress
                current_count = len(all_files)
                if total_files > 0:
                    progress = (current_count / total_files) * 100
                    self.logger.info(
                        f"Progress: {current_count}/{total_files} files ({progress:.1f}%)"
                    )
                else:
                    self.logger.info(f"Fetched {current_count} files so far")

                # Check if we have all files
                end_pos = range_info.get("end_pos", 0)
                if end_pos >= total_files - 1 or len(results) < limit:
                    self.logger.info(
                        f"Completed fetching all {len(all_files)} files from {repo_name}"
                    )
                    break

                # Prepare for next batch
                offset = end_pos + 1

                # Safety check to prevent infinite loops
                if offset > total_files:
                    self.logger.warning(
                        f"Offset ({offset}) exceeded total files ({total_files}), stopping"
                    )
                    break

            # Log some sample file paths to verify the query is working
            if all_files and self.logger.isEnabledFor(logging.DEBUG):
                sample_files = all_files[:5]  # First 5 files
                self.logger.debug(f"Sample file paths from {repo_name}:")
                for file_path, sha256 in sample_files:
                    self.logger.debug(f"  {file_path} (SHA256: {sha256[:16]}...)")

            # Log filtering statistics
            original_total = total_files if total_files else 0
            filtered_total = len(all_files)
            skipped_count = original_total - filtered_total

            if skipped_count > 0:
                self.logger.info(
                    f"Filtered out {skipped_count} auto-generated files during AQL processing"
                )

            self.logger.info(
                f"Successfully fetched {filtered_total} files from {repo_name} (after filtering)"
            )
            return all_files

        except Exception as e:
            self.logger.error(
                f"Failed to fetch files from {repo_name} on {server_id} using AQL: {str(e)}"
            )
            return []

    def should_skip_file(self, file_path: str) -> bool:
        """
        Check if a file should be skipped based on patterns and filename.

        Args:
            file_path: Path of the file to check

        Returns:
            True if file should be skipped, False otherwise
        """
        filename = os.path.basename(file_path)

        # Check if filename is in skipped files list
        if filename in self.skipped_files:
            return True

        # Check if path matches any skipped patterns
        for pattern in self.skipped_patterns:
            if re.match(pattern, file_path):
                return True

        return False

    def compare_repository(self, repo_name: str) -> Dict:
        """
        Compare files between source and target repository.

        Args:
            repo_name: Name of the repository to compare

        Returns:
            Dictionary containing comparison results
        """
        self.logger.info("-" * 60)
        self.logger.info(f"Comparing repository: {repo_name}")
        self.logger.info("-" * 60)

        try:
            # Create output directory for this repository
            repo_output_dir = Path(self.output_dir) / repo_name
            repo_output_dir.mkdir(exist_ok=True)

            # Get files from both servers
            source_files = self.get_repository_files(
                self.config["source_server"], repo_name
            )
            target_files = self.get_repository_files(
                self.config["target_server"], repo_name
            )

            # Convert to sets for comparison (file_path, sha256)
            source_set = set(source_files)
            target_set = set(target_files)

            # Find files that are in source but not in target (or have different checksums)
            diff_files = source_set - target_set

            # Extract just the file paths (files are already filtered during AQL processing)
            diff_file_paths = [file_path for file_path, sha256 in diff_files]

            # Generate results
            result = {
                "repo_name": repo_name,
                "total_source_files": len(source_files),
                "total_target_files": len(target_files),
                "total_diff_files": len(diff_files),
                "filtered_diff_files": len(diff_file_paths),
                "diff_files": diff_file_paths,
            }

            # Write results to files
            self._write_repository_results(repo_output_dir, result)

            self.logger.info(
                f"Repository {repo_name} comparison completed. "
                f"Found {len(diff_file_paths)} files to transfer"
            )

            return result

        except Exception as e:
            self.logger.error(f"Failed to compare repository {repo_name}: {str(e)}")
            return {"repo_name": repo_name, "error": str(e), "diff_files": []}

    def _write_repository_results(self, repo_output_dir: Path, result: Dict):
        """Write repository comparison results to files."""
        repo_name = result["repo_name"]
        diff_files = result.get("diff_files", [])

        if not diff_files:
            return

        # Write list of different files
        diff_file_path = repo_output_dir / f"{repo_name}.txt"
        with open(diff_file_path, "w") as f:
            f.write("-" * 50 + "\n")
            f.write(f"Files diff from source - Repo [{repo_name}]\n")
            f.write("-" * 50 + "\n")
            for i, file_path in enumerate(diff_files, 1):
                f.write(f"{i:6d}  {file_path}\n")

        # Write transfer commands
        transfer_file_path = repo_output_dir / "transfer.txt"
        with open(transfer_file_path, "w") as f:
            for file_path in diff_files:
                # Generate commands for download and upload
                download_cmd = self._generate_download_command(repo_name, file_path)
                upload_cmd = self._generate_upload_command(repo_name, file_path)
                cleanup_cmd = f'rm -f "{os.path.basename(file_path)}"'

                # Chain commands with cleanup for both curl and JFrog CLI
                f.write(f"{download_cmd} && {upload_cmd} && {cleanup_cmd}\n")

        # Append to full list log
        fulllist_path = Path(self.output_dir) / "fulllist.log"
        with open(fulllist_path, "a") as f:
            for i, file_path in enumerate(diff_files, 1):
                f.write(f"{i:6d}  {file_path}\n")

    def _generate_download_command(self, repo_name: str, file_path: str) -> str:
        """Generate download command (curl or JFrog CLI) to download file from source server."""
        if self.command_type == "jfrog":
            return self._generate_jfrog_download_command(repo_name, file_path)
        else:
            return self._generate_curl_download_command(repo_name, file_path)

    def _generate_curl_download_command(self, repo_name: str, file_path: str) -> str:
        """Generate curl command to download file from source server."""
        source_config = self.config["servers"][self.config["source_server"]]
        encoded_path = quote(file_path, safe="/")
        url = f"{source_config['url']}/{repo_name}/{encoded_path}"

        # Build curl command with authentication
        cmd_parts = ["curl", "-f", "-s", "-L"]

        if source_config.get("token"):
            cmd_parts.extend(
                ["-H", f"\"Authorization: Bearer {source_config['token']}\""]
            )
        elif source_config.get("username") and source_config.get("password"):
            cmd_parts.extend(
                ["-u", f"'{source_config['username']}:{source_config['password']}'"]
            )

        cmd_parts.extend(["-o", f'"{os.path.basename(file_path)}"', f'"{url}"'])

        return " ".join(cmd_parts)

    def _generate_jfrog_download_command(self, repo_name: str, file_path: str) -> str:
        """Generate JFrog CLI command to download file from source server."""
        source_server = self.config["source_server"]

        # JFrog CLI download command format: jf rt download --server-id=<server> --flat=true <source_path> <target_path>
        source_path = f"{repo_name}/{file_path}"
        target_path = os.path.basename(file_path)

        cmd_parts = [
            "jf",
            "rt",
            "download",
            f"--server-id={source_server}",
            "--flat=true",
            f'"{source_path}"',
            f'"{target_path}"',
        ]

        return " ".join(cmd_parts)

    def _generate_upload_command(self, repo_name: str, file_path: str) -> str:
        """Generate upload command (curl or JFrog CLI) to upload file to target server."""
        if self.command_type == "jfrog":
            return self._generate_jfrog_upload_command(repo_name, file_path)
        else:
            return self._generate_curl_upload_command(repo_name, file_path)

    def _generate_curl_upload_command(self, repo_name: str, file_path: str) -> str:
        """Generate curl command to upload file to target server."""
        target_config = self.config["servers"][self.config["target_server"]]
        encoded_path = quote(file_path, safe="/")
        url = f"{target_config['url']}/{repo_name}/{encoded_path}"

        # Build curl command with authentication
        cmd_parts = ["curl", "-f", "-s", "-X", "PUT"]

        if target_config.get("token"):
            cmd_parts.extend(
                ["-H", f"\"Authorization: Bearer {target_config['token']}\""]
            )
        elif target_config.get("username") and target_config.get("password"):
            cmd_parts.extend(
                ["-u", f"'{target_config['username']}:{target_config['password']}'"]
            )

        cmd_parts.extend(["-T", f'"{os.path.basename(file_path)}"', f'"{url}"'])

        return " ".join(cmd_parts)

    def _generate_jfrog_upload_command(self, repo_name: str, file_path: str) -> str:
        """Generate JFrog CLI command to upload file to target server."""
        target_server = self.config["target_server"]

        # JFrog CLI upload command format: jf rt upload --server-id=<server> <source_path> <target_path>
        source_path = os.path.basename(file_path)
        target_path = f"{repo_name}/{file_path}"

        cmd_parts = [
            "jf",
            "rt",
            "upload",
            f"--server-id={target_server}",
            f'"{source_path}"',
            f'"{target_path}"',
        ]

        return " ".join(cmd_parts)

    def run_comparison(self):
        """Run the full repository comparison process."""
        start_time = time.time()
        self.logger.info("Starting repository comparison process")

        try:
            # Get list of repositories to compare
            if self.repos_file:
                # When using repos file: read specific repositories and validate without type filtering
                # This allows comparing repositories of any type specified in the file
                requested_repos = self.read_repositories_from_file(self.repos_file)

                # Get available repositories from source server for validation (without type filter when using repos file)
                available_repos = self.get_repositories(
                    self.config["source_server"], use_type_filter=False
                )

                # Filter to only include existing repositories
                repositories = self.filter_existing_repositories(
                    requested_repos, available_repos
                )

                if not repositories:
                    self.logger.error(
                        "No valid repositories found from the specified file"
                    )
                    return

                self.logger.info(
                    f"Comparing {len(repositories)} repositories from file: {self.repos_file}"
                )
            else:
                # When not using repos file: get all repositories of specified type (default: local)
                repositories = self.get_repositories(
                    self.config["source_server"], use_type_filter=True
                )

                if not repositories:
                    self.logger.warning("No repositories found on source server")
                    return

                self.logger.info(
                    f"Comparing all {len(repositories)} repositories from source server"
                )

            # Initialize summary tracking
            total_files_to_transfer = 0
            completed_repos = 0
            failed_repos = 0

            # Process repositories with parallel workers
            max_workers = self.config.get("parallel_workers", 20)

            try:
                with SafeThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_repo = {
                        executor.submit(self.compare_repository, repo): repo
                        for repo in repositories
                    }

                    for future in concurrent.futures.as_completed(future_to_repo):
                        repo = future_to_repo[future]
                        try:
                            result = future.result()
                            if "error" not in result:
                                total_files_to_transfer += result.get(
                                    "filtered_diff_files", 0
                                )
                                completed_repos += 1
                            else:
                                failed_repos += 1
                        except Exception as e:
                            self.logger.error(
                                f"Repository {repo} generated an exception: {str(e)}"
                            )
                            failed_repos += 1
            except Exception as e:
                self.logger.error(f"Thread pool execution error: {str(e)}")
                # Continue with summary generation even if threading had issues

            # Generate summary
            self._generate_summary(
                repositories, total_files_to_transfer, completed_repos, failed_repos
            )

            elapsed_time = time.time() - start_time
            self.logger.info(
                f"Repository comparison completed in {elapsed_time:.2f} seconds"
            )
            self.logger.info(f"Total files to transfer: {total_files_to_transfer}")

        except Exception as e:
            self.logger.error(f"Repository comparison failed: {str(e)}")
            raise

    def _generate_summary(
        self, repositories: List[str], total_files: int, completed: int, failed: int
    ):
        """Generate summary report."""
        summary_path = Path(self.output_dir) / "summary.txt"

        with open(summary_path, "w") as f:
            f.write("=" * 60 + "\n")
            f.write("JFrog Artifactory Repository Comparison Summary\n")
            f.write("=" * 60 + "\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"Source Server: {self.config['source_server']}\n")
            f.write(f"Target Server: {self.config['target_server']}\n")
            f.write(f"Output Directory: {self.output_dir}\n")
            f.write("-" * 60 + "\n")
            f.write(f"Total Repositories Processed: {len(repositories)}\n")
            f.write(f"Successfully Completed: {completed}\n")
            f.write(f"Failed: {failed}\n")
            f.write(f"Total Files to Transfer: {total_files}\n")
            f.write("-" * 60 + "\n")
            f.write("\nTransfer Commands Usage:\n")
            f.write(
                '- Execute all transfers: find ./ -name "transfer.txt" -exec cat {} \\; | sh\n'
            )
            f.write(
                '- Count transfer commands per repo: find ./ -name "transfer.txt" -exec wc -l {} +\n'
            )
            f.write(
                '- Total transfer commands: find ./ -name "transfer.txt" -exec cat {} \\; | wc -l\n'
            )
            f.write(
                "- View diff files for specific repo: cat <repo-name>/<repo-name>.txt\n"
            )
            f.write("- View all diff files: cat fulllist.log\n")

        self.logger.info(f"Summary report generated: {summary_path}")


def main():
    """Main entry point for the script."""
    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    parser = argparse.ArgumentParser(
        description="Compare JFrog Artifactory repositories and generate transfer commands",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Using configuration file
  python compare_repos.py --config config.json

  # Compare only specific repositories from a file
  python compare_repos.py --config config.json --repos-file repos.txt

  # Compare remote repositories instead of local ones
  python compare_repos.py --config config.json --repo-type remote

  # Compare virtual repositories
  python compare_repos.py --config config.json --repo-type virtual

  # Generate JFrog CLI commands instead of curl commands
  python compare_repos.py --config config.json --command-type jfrog

  # Generate curl commands (default)
  python compare_repos.py --config config.json --command-type curl
        """,
    )

    parser.add_argument(
        "--config",
        "-c",
        required=True,
        help="Path to configuration file (JSON format) - REQUIRED",
    )

    parser.add_argument(
        "--repos-file",
        "-r",
        help="Path to text file containing repository names (one per line) to compare. If not provided, all repositories will be compared.",
    )

    parser.add_argument(
        "--repo-type",
        choices=["local", "remote", "virtual", "federated"],
        default="local",
        help="Type of repositories to fetch when not using --repos-file (default: local). Options: local, remote, virtual, federated",
    )

    parser.add_argument(
        "--command-type",
        choices=["curl", "jfrog"],
        default="curl",
        help="Type of transfer commands to generate (default: curl). Options: curl (curl commands), jfrog (JFrog CLI commands)",
    )

    args = parser.parse_args()

    try:
        comparer = ArtifactoryComparer(
            config_file=args.config,
            repos_file=args.repos_file,
            repo_type=args.repo_type,
            command_type=args.command_type,
        )
        comparer.run_comparison()

    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)
    finally:
        # Force garbage collection to help with thread cleanup
        import gc

        gc.collect()

        # Temporarily redirect stderr to suppress threading cleanup messages
        import os

        original_stderr = os.dup(2)
        null_fd = os.open(os.devnull, os.O_WRONLY)
        os.dup2(null_fd, 2)

        # Small delay to allow cleanup
        time.sleep(0.1)

        # Restore stderr
        os.dup2(original_stderr, 2)
        os.close(null_fd)
        os.close(original_stderr)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nOperation cancelled by user", file=sys.stderr)
        sys.exit(1)
    except SystemExit:
        raise
    except Exception as e:
        print(f"Unexpected error: {str(e)}", file=sys.stderr)
        sys.exit(1)
